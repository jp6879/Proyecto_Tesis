{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Entrenamiento de un modelo de red neuronal feedforward con validación cruzada K-fold\n",
    "* La red tiene como entrada las señales en componentes principales y tiene como salida lcm y σ con las que fueron generadas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias necesarias\n",
    "using Flux\n",
    "using Statistics\n",
    "using Flux: train!\n",
    "using Plots\n",
    "using Distributions\n",
    "using ProgressMeter\n",
    "using MultivariateStats\n",
    "using DataFrames\n",
    "using CSV\n",
    "using StatsPlots\n",
    "using LaTeXStrings\n",
    "using LinearAlgebra\n",
    "using PlotlyJS\n",
    "using CUDA\n",
    "using Random\n",
    "using Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traemos los parámetros necesarios\n",
    "include(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\Tesis_V1\\\\Proyecto_Tesis\\\\1-GeneracionDeDatos\\\\Parametros.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribucion de probabilidad log-normal se puede utilizar para añadir a la función de costo final, toma demasiado tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Pln(lcm::Float32, σ::Float32)\n",
    "    \"\"\"Función que calcula la probabilidad de un valor de lc dado un valor de lcm y un valor de σ con una distribución lognormal\n",
    "    Parametros: \n",
    "        lcm: valor de la media de la distribución lognormal\n",
    "        σ: valor de la desviación estandar de la distribución lognormal\n",
    "    Retorna:\n",
    "        Un arreglo con la probabilidad de cada valor de lc dado un valor de lcm y un valor de σ\n",
    "    \"\"\"\n",
    "    return [(exp(-(log(lc) - log(lcm))^2 / (2σ^2))) / (lc * σ * sqrt(2π)) for lc in lcs]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los datos a los que les realizamos PCA de las señales, hay dos archivos porque son dos generaciones de datos distintas, con diferente resolución\n",
    "\n",
    "# path_read = \"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\Datos\\\\Datos_PCA2\"\n",
    "path_read = \"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\Tesis_V1\\\\Proyecto_Tesis\\\\1-GeneracionDeDatos\\\\Datos_Final\\\\datos_PCA\"\n",
    "df_datasignals = CSV.read(path_read * \"\\\\df_PCA_Signals.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Funciones de pre procesamiento para escalar los datos y estandarizarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización Max-Min\n",
    "function MaxMin(data)\n",
    "    min_vals = minimum(data, dims=1)\n",
    "    max_vals = maximum(data, dims=1)\n",
    "    scaled_data = (data .- min_vals) ./ (max_vals .- min_vals)\n",
    "    return scaled_data\n",
    "\n",
    "end\n",
    "\n",
    "# Estandarización Z\n",
    "function Standarize(data)\n",
    "    mean_vals = mean(data, dims=1)\n",
    "    std_devs = std(data, dims=1)\n",
    "    standardized_data = (data .- mean_vals) ./ std_devs\n",
    "    return standardized_data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Metricas de validacion de la red neuronal, solo utilice RMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error\n",
    "function RMSE(predicted, real)\n",
    "    return sqrt(sum((predicted .- real).^2) / length(predicted))\n",
    "end\n",
    "\n",
    "# Mean Absolute Error\n",
    "function MAE(predicted, real)\n",
    "    return sum(abs.(predicted .- real)) / length(predicted)\n",
    "end\n",
    "\n",
    "# R2 score\n",
    "function R2_score(predicted, real)\n",
    "    return 1 - sum((predicted .- real).^2) / sum((real .- mean(real)).^2)\n",
    "end\n",
    "\n",
    "# Realetive Root Mean Squared Error\n",
    "function RRMSE(predicted, real)\n",
    "    return sqrt(mean((predicted .- real).^2)) / mean(real)\n",
    "end\n",
    "\n",
    "# Relative Mean Absolute Error\n",
    "function RMAE(predicted, real)\n",
    "    return mean(abs.(predicted .- real)) / mean(real)\n",
    "end\n",
    "\n",
    "# Mean Absolute Percentaje Error\n",
    "function MAPE(predicted, real)\n",
    "    return mean(abs.((predicted .- real) ./ real))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizaciones L1 y L2\n",
    "pen_l2(x::AbstractArray) = Float32.(sum(abs2, x) / 2)\n",
    "pen_l1(x::AbstractArray) = Float32.(sum(abs, x) / 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos la técnica k-fold de validación cruzada para prevenir el overfitting de la red neuronal\n",
    "# Definimos el número de folds\n",
    "folds = 5\n",
    "step_valid = 20\n",
    "num_datos = Int(size(df_datasignals, 1))\n",
    "\n",
    "# Guardamos los datos de validacion de cada NN en cada fold\n",
    "out_of_sample_data = []\n",
    "out_of_sample_pred = []\n",
    "\n",
    "# Guardamos la metrica de validación de cada NN en cada fold\n",
    "scores_MAE = []\n",
    "\n",
    "# Primero sacamos los datos de testing de los datos de señales, estos seran un 3er conjunto de datos que no se usara para entrenar ni validar la red\n",
    "df_datasignals_out = df_datasignals[6 + 1:step_valid:num_datos,:]\n",
    "# Datos de Testing\n",
    "df_datasignals_minus_out = df_datasignals[setdiff(1:num_datos, 6 + 1:step_valid:num_datos),:]\n",
    "# Nuevo numero de datos que tenemos para entrenamiento + validacion\n",
    "num_datos_new = Int(size(df_datasignals_minus_out, 1))\n",
    "\n",
    "for k in 1:folds\n",
    "    # Usamos 5 conjuntos disjuntos de datos de validación del 5% total de los datos para cada fold\n",
    "\n",
    "    datasignals_valid = Float32.(Matrix(df_datasignals_minus_out[k^2 + 10:step_valid:num_datos_new,1:3])')\n",
    "    datasignals = Float32.(Matrix(df_datasignals_minus_out[setdiff(1:num_datos_new, k^2 + 10:step_valid:num_datos_new),1:3])')\n",
    "\n",
    "    σ_valid = df_datasignals_minus_out[k^2 + 10:step_valid:num_datos_new,4]\n",
    "    lcm_valid = df_datasignals_minus_out[k^2 + 10:step_valid:num_datos_new,5]\n",
    "    \n",
    "    σ_col = df_datasignals_minus_out[setdiff(1:num_datos_new, k^2 + 10:step_valid:num_datos_new),4]\n",
    "    lcm_col = df_datasignals_minus_out[setdiff(1:num_datos_new, k^2 + 10:step_valid:num_datos_new),5]\n",
    "    \n",
    "    dataparams = hcat(lcm_col, σ_col)'\n",
    "    dataparams_valid = hcat(lcm_valid, σ_valid)'\n",
    "\n",
    "    # Definimos la red neuronal\n",
    "    model = Chain(\n",
    "        Dense(3, 32, swish),\n",
    "        Dense(32, 64, relu),\n",
    "        Dense(64, 32, swish),\n",
    "        Dense(32, 16, relu),\n",
    "        Dense(16, 2, softplus),\n",
    "    )\n",
    "\n",
    "    # Función de loss\n",
    "    function loss(x,y)\n",
    "        return Flux.mse(model(x), y)\n",
    "    end\n",
    "\n",
    "    # Definimos el metodo de aprendizaje y la tasa de aprendizaje\n",
    "    η = 1e-4\n",
    "    opt = ADAM(η)\n",
    "\n",
    "    # Definimos el número de épocas\n",
    "    epochs = 3000\n",
    "\n",
    "    # Definimos el tamaño del batch\n",
    "    batch_size = 64\n",
    "\n",
    "    # Usamos dataloader para cargar los datos\n",
    "    data = Flux.DataLoader((datasignals, dataparams), batchsize = batch_size, shuffle = true)\n",
    "    data_valid = Flux.DataLoader((datasignals_valid, dataparams_valid), batchsize = batch_size, shuffle = true)\n",
    "\n",
    "    # Definimos una funcion de callback para ver el progreso del entrenamiento\n",
    "    iter = 0\n",
    "    cb = function()\n",
    "        global iter += 1\n",
    "        if iter % length(data) == 0\n",
    "            epoch = iter ÷ length(data)\n",
    "            if epoch % 500 == 0\n",
    "                actual_loss = loss(data.data[1], data.data[2])\n",
    "                actual_valid_loss = loss(data_valid.data[1], data_valid.data[2])\n",
    "                println(\"Epoch $epoch || Loss = $actual_loss || Valid Loss = $actual_valid_loss\")\n",
    "            end\n",
    "            # push!(losses, actual_loss)\n",
    "            # push!(losses_valid, actual_valid_loss)\n",
    "        end\n",
    "    end;\n",
    "\n",
    "    # Entrenamos la red neuronal con el loss mse variando la tasa de aprendizaje cada 500 épocas\n",
    "    for epoch in 1:epochs\n",
    "        Flux.train!(loss, Flux.params(model, opt), data, opt, cb=cb)\n",
    "        if epoch % 500 == 0\n",
    "            η = η * 0.2\n",
    "            opt = ADAM(η)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Predicción de la red en la validacion\n",
    "    predictions_valid = model(datasignals_valid)\n",
    "\n",
    "    # Métricas de validación de la red\n",
    "    RMAE_valid = RMAE(predictions_valid, dataparams_valid)\n",
    "\n",
    "    push!(scores_MAE, RMAE_valid)\n",
    "\n",
    "    # Guardamos los datos de validación y las predicciones de la red\n",
    "    push!(out_of_sample_data, dataparams_valid)\n",
    "    push!(out_of_sample_pred, predictions_valid)\n",
    "\n",
    "    println(\"Fold $k terminado con score de validación RMAE = $RMAE_valid\")\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora sacamos el promedio de la metrica de validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"El promedio de la metrica de validación es $(mean(scores_MAE)) y el desvio estandar es $(std(scores_MAE))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos entrenar el modelo tranquilamente sabiendo que no depende de la partición de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de datos total (sin tener en cuenta testing)\n",
    "num_datos = Int(size(df_datasignals_minus_out, 1))\n",
    "# Pasos en los que tomamos datos de validación\n",
    "step_valid = 20\n",
    "\n",
    "# Definimos el número de fold que usaremos para testing\n",
    "k = 7\n",
    "# Usamos 5 conjuntos disjuntos de datos de validación del 5% total de los datos para cada fold\n",
    "datasignals_test = Float32.(Matrix(df_datasignals_out[:,1:3])')\n",
    "datasignals_valid = Float32.(Matrix(df_datasignals_minus_out[k^2:step_valid:num_datos_new,1:3])')\n",
    "datasignals = Float32.(Matrix(df_datasignals_minus_out[setdiff(1:num_datos_new, k^2:step_valid:num_datos_new),1:3])')\n",
    "\n",
    "# Identificamos con lcm y σ de los datos de validación, entrenamiento y testing\n",
    "σ_test = df_datasignals_out[:,4]\n",
    "lcm_test = df_datasignals_out[:,5]\n",
    "\n",
    "σ_valid = df_datasignals_minus_out[k^2:step_valid:num_datos_new,4]\n",
    "lcm_valid = df_datasignals_minus_out[k^2:step_valid:num_datos_new,5]\n",
    "\n",
    "σ_col = df_datasignals_minus_out[setdiff(1:num_datos_new, k^2:step_valid:num_datos_new),4]\n",
    "lcm_col = df_datasignals_minus_out[setdiff(1:num_datos_new, k^2:step_valid:num_datos_new),5]\n",
    "\n",
    "dataparams = hcat(lcm_col, σ_col)'\n",
    "dataparams_valid = hcat(lcm_valid, σ_valid)'\n",
    "dataparams_test = hcat(lcm_test, σ_test)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(3, 32, swish),\n",
    "    Dense(32, 64, relu),\n",
    "    Dense(64, 32, swish),\n",
    "    Dense(32, 16, relu),\n",
    "    Dense(16, 2, softplus),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Funciones loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de loss\n",
    "function loss(x,y)\n",
    "    y_hat = model(x)\n",
    "    return Flux.mse(y_hat, y)\n",
    "end\n",
    "\n",
    "# Loss compuesto\n",
    "function composed_loss(x,y)\n",
    "    y_hat = model(x)\n",
    "    Pln_predicted = Pln.(y_hat[1,:], y_hat[2,:])\n",
    "    Pln_real = Pln.(y[1,:], y[2,:])\n",
    "    return mean(Flux.mse.(Pln_predicted,Pln_real)) + Flux.mse(y_hat, y)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Usamos dataloader para cargar los datos\n",
    "data = Flux.DataLoader((datasignals, dataparams), batchsize = batch_size, shuffle = true) \n",
    "data_valid = Flux.DataLoader((datasignals_valid, dataparams_valid), batchsize = batch_size, shuffle = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de la red neuronal\n",
    "params = Flux.params(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Callback para guardar el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el vector donde guardamos la pérdida\n",
    "losses = []\n",
    "losses_valid = []\n",
    "\n",
    "# Definimos una funcion de callback para ver el progreso del entrenamiento\n",
    "iter = 0\n",
    "cb = function()\n",
    "    global iter += 1\n",
    "    if iter % length(data) == 0\n",
    "        epoch = iter ÷ length(data)\n",
    "        actual_loss = loss(data.data[1], data.data[2])\n",
    "        actual_valid_loss = loss(data_valid.data[1], data_valid.data[2])\n",
    "        if epoch%100 == 0\n",
    "            println(\"Epoch $epoch || Loss = $actual_loss || Valid Loss = $actual_valid_loss\")\n",
    "        end\n",
    "        push!(losses, actual_loss)\n",
    "        push!(losses_valid, actual_valid_loss)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dejamos otro callback por si utilizamos el loss compuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_composed = []\n",
    "losses_composed_valid = []\n",
    "cb2 = function()\n",
    "    global iter += 1\n",
    "    epoch = iter ÷ length(data)\n",
    "    actual_loss = composed_loss(data.data[1], data.data[2])\n",
    "    actual_valid_loss = composed_loss(data_valid.data[1], data_valid.data[2])\n",
    "    println(\"Epoch $epoch || Loss = $actual_loss || Valid Loss = $actual_valid_loss\")\n",
    "    push!(losses_composed, actual_loss)\n",
    "    push!(losses_composed_valid, actual_valid_loss)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimización, tasa de aprendizaje y número de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = 1e-5\n",
    "opt = ADAM(η)\n",
    "epochs = 5000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1:epochs\n",
    "    Flux.train!(loss, Flux.params(model, opt), data, opt, cb=cb)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predicciones de la red en todos los conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones de la red\n",
    "predictions = model(datasignals)\n",
    "predictions_valid = model(datasignals_valid)\n",
    "predictions_test = model(datasignals_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Medidas de errores globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2_train = R2_score(predictions, dataparams)\n",
    "# RMSE_train = RMSE(predictions, dataparams)\n",
    "MAE_train = MAE(predictions, dataparams)\n",
    "# RRMSE_train = RRMSE(predictions, dataparams)\n",
    "RMAE_train = RMAE(predictions, dataparams)\n",
    "\n",
    "# R2_valid = R2_score(predictions_valid, dataparams_valid)\n",
    "# RMSE_valid = RMSE(predictions_valid, dataparams_valid)\n",
    "MAE_valid = MAE(predictions_valid, dataparams_valid)\n",
    "# RRMSE_valid = RRMSE(predictions_valid, dataparams_valid)\n",
    "RMAE_valid = RMAE(predictions_valid, dataparams_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Medidas de errores puntuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(predictions[1,:])\n",
    "N_valid = length(predictions_valid[1,:])\n",
    "N_test = length(predictions_test[1,:])\n",
    "\n",
    "RMAE_scores = zeros(N)\n",
    "RMAE_scores_valid = zeros(N_valid)\n",
    "RMAE_scores_test = zeros(N_test)\n",
    "MAE_scores = zeros(N)\n",
    "MAE_scores_valid = zeros(N_valid)\n",
    "MAE_scores_test = zeros(N_test)\n",
    "\n",
    "for i in 1:N\n",
    "    # RMSE_scores[i] = RMSE(predictions[:,i], dataparams[:,i])\n",
    "    MAE_scores[i] = MAE(predictions[:,i], dataparams[:,i])\n",
    "    # RRMSE_scores[i] = RRMSE(predictions[:,i], dataparams[:,i])\n",
    "    RMAE_scores[i] = RMAE(predictions[:,i], dataparams[:,i])\n",
    "end\n",
    "\n",
    "for i in 1:N_valid\n",
    "    # RMSE_scores_valid[i] = RMSE(predictions_valid[:,i], dataparams_valid[:,i])\n",
    "    MAE_scores_valid[i] = MAE(predictions_valid[:,i], dataparams_valid[:,i])\n",
    "    # RRMSE_scores_valid[i] = RRMSE(predictions_valid[:,i], dataparams_valid[:,i])\n",
    "    RMAE_scores_valid[i] = RMAE(predictions_valid[:,i], dataparams_valid[:,i])\n",
    "end\n",
    "\n",
    "for i in 1:N_test\n",
    "    MAE_scores_test[i] = MAE(predictions_test[:,i], dataparams_test[:,i])\n",
    "    RMAE_scores_test[i] = RMAE(predictions_test[:,i], dataparams_test[:,i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluamos la red neuronal con los datos de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 0\n",
    "worst = 0\n",
    "for i in 1:N_test\n",
    "    if minimum(RMAE_scores_test) == RMAE_scores_test[i]\n",
    "        println(\"El mínimo RMAE es $(RMAE_scores_test[i]) y se encuentra en la posición $i\")\n",
    "        best = i\n",
    "    end\n",
    "    if maximum(RMAE_scores_test) == RMAE_scores_test[i]\n",
    "        println(\"El máximo RMAE es $(RMAE_scores_test[i]) y se encuentra en la posición $i\")\n",
    "        worst = i\n",
    "    end\n",
    "end\n",
    "\n",
    "P_real_best = Pln.(dataparams_test[1,best], dataparams_test[2,best])\n",
    "P_predict_best = Pln.(predictions_test[1,best], predictions_test[2,best])\n",
    "\n",
    "P_real_worst = Pln.(dataparams_test[1,worst], dataparams_test[2,worst])\n",
    "P_predict_worst = Pln.(predictions_test[1,worst], predictions_test[2,worst])\n",
    "\n",
    "Plots.plot(lcs, P_real_best, label = L\"Real $l_{cm} = $\" * \" $(dataparams_test[1,best]) \" * L\"$\\mu m$\" * L\" $σ = $\" * \" $(dataparams_test[2,best])\", xlabel = L\"l_c\", ylabel = L\"P(l_c)\", lw = 4, legend=:best, tickfontsize=11, labelfontsize=13, legendfontsize=9, framestyle =:box, gridlinewidth=1, xminorticks=10, yminorticks=10, right_margin=5mm)\n",
    "Plots.plot!(lcs, P_predict_best, label = L\"Predicción $l_{cm} = $\" * \" $(round(predictions_test[1,best],digits = 4)) \" * L\"$\\mu m$\" * L\" $σ =$\" * \" $(round(predictions_test[2,best],digits = 4))\",lw = 2, legend=:best, tickfontsize=11, labelfontsize=13, legendfontsize=9, framestyle =:box, gridlinewidth=1, xminorticks=10, yminorticks=10, right_margin=5mm)\n",
    "# Plots.savefig(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\Series\\\\Graficos\\\\P_real_vs_P_predS2.png\")\n",
    "# Plots.savefig(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\Series\\\\Graficos\\\\P_real_vs_P_predS2.pdf\")\n",
    "\n",
    "Plots.plot(lcs, P_real_worst, label = L\"Real $l_{cm} = $\" * \" $(dataparams_test[1,worst]) \" * L\"$\\mu m$ \" * L\" $σ = $\" * \" $(dataparams_test[2,worst])\", xlabel = L\"l_c\", ylabel = L\"P(l_c)\", lw = 4, legend=:best, tickfontsize=11, labelfontsize=13, legendfontsize=9, framestyle =:box, gridlinewidth=1, xminorticks=10, yminorticks=10, right_margin=5mm)\n",
    "Plots.plot!(lcs, P_predict_worst, label = L\"Predicción $l_{cm} = $\" * \" $(round(predictions_test[1,worst],digits = 4)) \" * L\"$\\mu m$\" * L\" $σ = $\" * \" $(round(predictions_test[2,worst],digits = 4))\", xlabel = L\"l_c\", ylabel = L\"P(l_c)\", lw = 2, legend=:best, tickfontsize=11, labelfontsize=13, legendfontsize=9, framestyle =:box, gridlinewidth=1, xminorticks=10, yminorticks=10, right_margin=5mm)\n",
    "xlims!(0, 3)\n",
    "# Plots.savefig(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\Series\\\\Graficos\\\\Worst_P_real_vs_P_predS2.pdf\")\n",
    "# Plots.savefig(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\Series\\\\Graficos\\\\Worst_P_real_vs_P_predS2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plots de los errores de la predicion de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_error = PlotlyJS.scatter(\n",
    "    x = datasignals_valid[1,1:end],\n",
    "    y = datasignals_valid[2,1:end],\n",
    "    mode = \"markers\",\n",
    "    hoverinfo = \"text\",\n",
    "    hovertext = RMAE_scores[1:end],\n",
    "    marker = attr(\n",
    "        color = RMAE_scores[1:end],  # Use the color_vector for color mapping\n",
    "        colorscale = \"Hot\",  # Choose a predefined colormap (e.g., \"Viridis\")\n",
    "        colorbar_title = \"RMAE\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "params_layout = Layout(\n",
    "    title = \"Datos de validación\",\n",
    "    xaxis = attr(title = \"PC1\"),\n",
    "    yaxis = attr(title = \"PC2\"),\n",
    "    font = attr(size = 15),\n",
    ")\n",
    "\n",
    "params_plot = PlotlyJS.plot([params_error], params_layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
