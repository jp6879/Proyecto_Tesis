{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Neural Ordinary Differential Equations para la predicción de una serie temporal\n",
    "\n",
    "* Construimos una Neural ODE con el objetivo de predecir señales de Hahn a partir de una cantidad reducida de datos. Como las señales de Hahn se cruzan entre sí tanto en el mismo punto inicial como en diferentes puntos, utilizamos un embedding para representar las señales de Hahn en un espacio de características de mayor dimensión. Este embedding está compuesto por la señal de Hahn original y la derivada de la señal de Hahn. Luego, utilizamos una Neural ODE para predecir la señal de Hahn en el futuro.\n",
    "\n",
    "* Para tener un conjunto de señales sencillos para entrenar y chequear que funciona, utilizo señales de Hahn simuladas con un mismo $\\sigma = 1$ y ver si la red puede predecir 50 señales de estas. Para un proceso mas complejo se buscaria entrenar con multiples señales de Hahn con diferentes $\\sigma$.\n",
    "\n",
    "* De manera mas general seleccionamos un subconjunto reducido de señales que sea representativo aproximadamente de todas las señales que tenemos simuladas en el dataset. El cual nos daría una red que cumpliría con las predicciones de los datos de manera general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: train!\n",
    "using DataFrames\n",
    "using CSV\n",
    "using DifferentialEquations\n",
    "using SciMLSensitivity\n",
    "using ComponentArrays, Optimization, OptimizationOptimJL, OptimizationFlux\n",
    "using Interpolations\n",
    "using OrdinaryDiffEq\n",
    "using IterTools: ncycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los parámetros fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros fijos\n",
    "N = 5000\n",
    "time_sample_lenght_long = 1000\n",
    "time_sample_lenght_short = 100\n",
    "\n",
    "# Rango de tamaños de compartimientos en μm\n",
    "l0 = 0.01\n",
    "lf = 45\n",
    "\n",
    "# Tiempo final de simulación en s\n",
    "tf = 1\n",
    "\n",
    "# Ahora generamos los datos para eso necesitamos hacer el sampling de los lc y los t\n",
    "lc = range(l0, lf, length = N) # Esto nos da un muestreo de 0,008998 μm en lc\n",
    "t_short = collect(range(0, 0.1, length = time_sample_lenght_short)) # Muestreo corto de 0.1 ms\n",
    "t_long = collect(range(0.1, 1, length = time_sample_lenght_long)) # Muestreo largo de 10 ms\n",
    "\n",
    "# Concatenamos los tiempos para tener un muestreo completo \n",
    "t = vcat(t_short, t_long)\n",
    "\n",
    "# Parametros que se varian, estos se corresponden a la mediana y la desviación estándar de la distribución de tamaños de compartimientos lcms en μm y σs adimensionales\n",
    "\n",
    "lcms = 0.5:0.01:6\n",
    "σs = 0.01:0.01:1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilidades para cargar las señales a partir de un rango de índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer las señales\n",
    "function GetSignals(path_read)\n",
    "    dataSignals = CSV.read(path_read * \"/dataSignals.csv\", DataFrame)\n",
    "    dataSignals = Matrix(dataSignals)\n",
    "    return dataSignals\n",
    "end\n",
    "\n",
    "# Funcion que tomas algunas señales para utilizarlas en el entrenamiento de la NODE, recibe el numero de señales que quermos tomar y de donde las queremos tomar\n",
    "function Get_Signals_Test(rango, path_read, muestreo_corto, muestreo_largo)\n",
    "    # Leemos las señales desde el archivo\n",
    "    dataSignals = Float32.(GetSignals(path_read))\n",
    "    # Tomamos únicamente el rango de señales que vamos a tomar para esta red\n",
    "    Signals_test = Float32.(Matrix(dataSignals[:,rango]'))\n",
    "    # Tomamos un subconjunto de tiempos para agilizar el entrenamiento, tiempos cortos es decir el muestreo hasta 0.1 s\n",
    "    Signals_test_short = Signals_test[:,1:muestreo_corto:1000]\n",
    "    # Tiempos largos es decir el muestreo después de 0.1 s\n",
    "    Signals_test_long = Signals_test[:,1001:muestreo_largo:end]\n",
    "    # Concatenamos las señales con el nuevo muestreo para agilizar el entrenamiento\n",
    "    Signals_test = hcat(Signals_test_short, Signals_test_long)\n",
    "    return Signals_test\n",
    "end\n",
    "\n",
    "###################################################################################\n",
    "# Función que idientifca las señales utilizadas, es decir los parámetros lcm y σ\n",
    "function Get_Signals_Test_Parameters(numSignals,lcms,σs)\n",
    "    dim1 = dimlcm = length(lcms)\n",
    "    dim2 = dimσ = length(σs)\n",
    "\n",
    "    # Inicializamos los arreglos que vamos a utilizar\n",
    "    column_lcm = zeros(dim1*dim2)\n",
    "    column_σs = zeros(dim1*dim2)\n",
    "    # Convertimos los iterables de lcm y sigma con los cuales generamos las señales en arreglos\n",
    "    aux_lcm = collect(lcms)\n",
    "    aux_σs = collect(σs)\n",
    "\n",
    "    # Asignamos los valores de lcm y sigma ordenados tal cual están las señales, es decir los datos comienzan en σ = 0.1 y lcm = 0.5 y va en aumento de lcm hasta 6 y luego pasa al siguiente σ\n",
    "    for i in 1:dim1\n",
    "        for j in 1:dim2\n",
    "            column_lcm[(i - 1)*dim2 + j] = aux_lcm[i]\n",
    "            column_σs[(i - 1)*dim2 + j] = aux_σs[j]\n",
    "        end\n",
    "    end\n",
    "    # Con esto ya tenemos los arrelgos de los parámetros que se utilizaron para generar las señales\n",
    "    column_lcm_test = column_lcm[1:Int(length(column_lcm)/numSignals):end]\n",
    "    column_σs_test = column_σs[1:Int(length(column_σs)/numSignals):end]\n",
    "    \n",
    "    return column_lcm_test, column_σs_test\n",
    "end\n",
    "\n",
    "# Función que devuelve señales de prueba, sus derivadas y los parámetros con los que se generaron\n",
    "function Get_Signals_Data_Training(path_read, rango, lcms, sigmas, muestreo_corto, muestreo_largo)\n",
    "    # Obtenemos las señales que vamos a utilizar\n",
    "    Signals_test = Get_Signals_Test(rango, path_read, muestreo_corto, muestreo_largo)\n",
    "    # Obtenemos los parámetros con los que se generaron las señales\n",
    "    column_lcm_test, column_sigmass_test = Get_Signals_Test_Parameters(rango,lcms,sigmas)\n",
    "    # Calculamos las derivadas de las señales\n",
    "    Signals_test_derivadas = zeros(size(Signals_test))\n",
    "    for i in 1:size(Signals_test)[1]\n",
    "        Signals_test_derivadas[i,:] = derivate_signals(t,Signals_test[i,:])\n",
    "    end\n",
    "    Signals_test_derivadas = Float32.(Matrix(Signals_test_derivadas'))\n",
    "    # Normalizamos las derivadas\n",
    "    for i in 1:size(Signals_test)[1]\n",
    "        Signals_test_derivadas[:,i] = Signals_test_derivadas[:,i] ./ maximum(abs.(Signals_test_derivadas[:,i]))\n",
    "    end\n",
    "    return Signals_test, Signals_test_derivadas, column_lcm_test, column_sigmass_test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para las derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a hacer una función que nos permita calcular las derivadas de las señales\n",
    "# Para esto vamos a usar diferencias finitas centradas\n",
    "# La función recibe como argumentos el arreglo de señales y el arreglo de tiempos\n",
    "# La función regresa un arreglo de derivadas de las señales\n",
    "\n",
    "function derivate_signals(t,signal)\n",
    "    # Calcula el tamaño de la ventana\n",
    "    w = 1\n",
    "    # Calcula el tamaño de la señal\n",
    "    n = length(signal)\n",
    "    # Inicializa el arreglo de derivadas\n",
    "    derivadas = zeros(n)\n",
    "    for i in 1:n\n",
    "        # Encuentra los índices de la ventana\n",
    "        inicio = max(1, i-w)\n",
    "        final = min(n, i+w)\n",
    "        # Utiliza diferencias finitas centradas si es posible\n",
    "        if inicio != i && final != i\n",
    "            derivadas[i] = (signal[final] - signal[inicio]) / (t[final] - t[inicio])\n",
    "        elseif inicio == i\n",
    "            # Diferencia hacia adelante si estamos en el comienzo del arreglo\n",
    "            derivadas[i] = (signal[i+1] - signal[i]) / (t[i+1] - t[i])\n",
    "        else\n",
    "            # Diferencia hacia atrás si estamos al final del arreglo\n",
    "            derivadas[i] = (signal[i] - signal[i-1]) / (t[i] - t[i-1])\n",
    "        end\n",
    "    end\n",
    "    return derivadas\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de las NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que crea el modelo de la red neuronal que va a estar dentro de la ODE\n",
    "function create_model(layers::Vector{Int}, activation)\n",
    "    \"\"\"Función que crea el modelo de la red neuronal que va a estar dentro de la ODE\n",
    "\n",
    "    Parameters:\n",
    "        layers::Vector{Int} : Vector con las capas de la red neuronal\n",
    "        activation : Función de activación de la red neuronal para todas las capas\n",
    "\n",
    "    Returns:\n",
    "        function Chain : Red neuronal creada\n",
    "    \"\"\"\n",
    "    # Creamos la red neuronal con las capas y la activación que nos pasan\n",
    "    activations = [activation for i in 1:length(layers) - 2]\n",
    "    # Regresamos la red neuronal\n",
    "    return Chain([Dense(layers[i], layers[i+1], activations[i]) for i in 1:length(layers) - 2]..., Dense(layers[end-1], layers[end]))\n",
    "end\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Función que entrena la NODE con mini-batchs\n",
    "function Train_Neural_ODE(nn, U0, extra_parameters ,num_epochs, train_loader, opt, eta, Signals, Signals_forecast, t, tforecast)\n",
    "    \"\"\"Función que entrena la NODE con mini-batchs\n",
    "    Parameters:\n",
    "        nn : Red neuronal que va a estar dentro de la ODE\n",
    "        U0 : Condiciones iniciales de la ODE\n",
    "        extra_parameters : Parámetros extra que se le pasan a la red neuronal\n",
    "        num_epochs : Número de epochs que se va a entrenar la red\n",
    "        train_loader : DataLoader con los datos de entrenamiento\n",
    "        opt : Optimizador que se va a utilizar\n",
    "        eta : Learning rate\n",
    "        Signals : datos de las señales que se van a utilizar para entrenar la red\n",
    "        Signals_forecast : datos de las señales que se buscan predecir\n",
    "        t : Tiempos de las señales recortado que se usa para entrenar la red\n",
    "        tforecast : Tiempos de las señales que se van a predecir\n",
    "    Returns:\n",
    "        loss : Arreglo con el loss de la red neuronal\n",
    "        p : Parámetros de la red neuronal\n",
    "        loss_forecast : Arreglo con el loss de la red neuronal para los datos de las señales que se predicen\n",
    "    \"\"\"\n",
    "    # Tiempo sobre el cual resolver\n",
    "    tspan = (0f0, 1f0)\n",
    "\n",
    "    # Parametrizar indices para poder acceder a los parametros extras de la red neuronal durante el entrenamiento y predicción\n",
    "    f(x,p) = round(Int, x * (length(p) - 1)) + 1\n",
    "\n",
    "    # Para entrenar la red tenemos que extraer los parametros de la red neuronal en su condicion inicial\n",
    "    p, re = Flux.destructure(nn) \n",
    "\n",
    "    # Optimizardor que se va a utilizar\n",
    "    opt = opt(eta)\n",
    "\n",
    "    # Función que resuelve la ODE con los parametros extra y las condiciones iniciales que instanciemos y nos regresa la solución en un arreglo\n",
    "    function predict_NeuralODE(u0, parametros, time_batch)\n",
    "        # dSdt = NN(S, parametros_extra) \n",
    "        function dSdt(u, p, t; parametros_extra = parametros)\n",
    "            # Selecciona el indice de los parametros extra en el tiempo t\n",
    "            indx = f(t, parametros)\n",
    "            # Selecciona los parametros extra en el tiempo t\n",
    "            parametros_actuales = parametros[indx]\n",
    "            # Concatena los el valor de S(t) con los parametros extra en el tiempo t\n",
    "            entrada_red = vcat(u, parametros_actuales)\n",
    "            # Regresa la salida de la red neuronal reconstruida con los parámetros p y esta vez con los parametros extra en el tiempo t\n",
    "            return re(p)(entrada_red) \n",
    "        end\n",
    "\n",
    "        # Definimos el problema de la ODE\n",
    "        prob = ODEProblem(dSdt, u0, tspan)\n",
    "\n",
    "        # Resolvemos la ODE y la devolvemos\n",
    "        return Array(solve(prob, Tsit5(), dtmin=1e-9 , u0 = u0, p = p, saveat = time_batch, reltol = 1e-7, abstol = 1e-7))\n",
    "    end\n",
    "\n",
    "    # Función que predice las señales para un conjunto de condiciones iniciales\n",
    "    function Predict_Singals(U0, parametros_extra, time_batch)\n",
    "        Predicted_Signals = zeros(size(time_batch))\n",
    "        # Iteramos sobre las condiciones iniciales y las derivadas de las señales\n",
    "        for i in 1:length(U0)\n",
    "            u0 = Float32[U0[i]]\n",
    "            predicted_signal = predict_NeuralODE(u0, parametros_extra[:, i], time_batch)[1, :]\n",
    "            Predicted_Signals = hcat(Predicted_Signals, predicted_signal)\n",
    "        end\n",
    "        Predicted_Signals[:,2:end]\n",
    "    end\n",
    "\n",
    "    # Función de loss que vamos a minimizar, recibe un batch de señales y un batch de tiempos\n",
    "    function loss_node(batch, time_batch)\n",
    "        y = Predict_Singals(U0, extra_parameters, time_batch)\n",
    "        return Flux.mse(y, batch')\n",
    "    end\n",
    "    \n",
    "    # Función de callback para guardar el loss cada epoch\n",
    "    global iter = 0\n",
    "    loss = []\n",
    "    loss_forecast = []\n",
    "    callback = function ()\n",
    "        global iter += 1\n",
    "        if iter % (length(train_loader)) == 0\n",
    "            epoch = Int(iter / length(train_loader))\n",
    "            actual_loss = loss_node(Signals, t)\n",
    "            forecast_loss = loss_node(Signals_forecast, tforecast)\n",
    "            # println(\"Epoch = $epoch || Loss: $actual_loss || Loss Forecast: $forecast_loss\")\n",
    "            push!(loss, actual_loss)\n",
    "            push!(loss_forecast, forecast_loss)\n",
    "        end\n",
    "        return false\n",
    "    end\n",
    "\n",
    "    # Entrenamos la red neuronal con mini-batchs\n",
    "    Flux.train!(loss_node, Flux.params(p), ncycle(train_loader, num_epochs), opt, cb = callback)\n",
    "    \n",
    "    # Devolvemos el loss final y los parametros de la red neuronal\n",
    "    return loss, p, loss_forecast\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función main donde se exploran distintas redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "function main()\n",
    "    # Arquitecturas que vamos a utilizar\n",
    "    architectures = [\n",
    "        [[2, 8, 1], relu], # Una capa oculta\n",
    "        [[2, 8, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 8, 1], swish], # Misma con activación swish\n",
    "        \n",
    "        [[2, 16, 16, 1], relu], # Dos capas ocultas\n",
    "        [[2, 16, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 16, 16, 1], swish], # Misma con activación swish\n",
    "        \n",
    "        [[2, 32, 64, 16, 1], relu], # Tres capas ocultas\n",
    "        [[2, 32, 64, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 32, 64, 16, 1], swish], # Misma con activación swish\n",
    "\n",
    "        [[2, 50, 64, 16, 1], relu], # Tres capas ocultas con mas neuronas\n",
    "        [[2, 50, 64, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 50, 64, 16, 1], swish], # Misma con activación swish\n",
    "        \n",
    "        [[2, 32, 64, 64, 32, 16, 1], relu], # Cinco capas ocultas\n",
    "        [[2, 32, 64, 64, 32, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 32, 64, 64, 32, 16, 1], swish], # Misma con activación swish\n",
    "        ]\n",
    "\n",
    "    # Optimizadores que vamos a utilizar\n",
    "    optimizers = [opt for opt in [AdamW, RMSProp]]\n",
    "\n",
    "    # Numero de mini-batchs que vamos a utilizar\n",
    "    batchs_size = [5, 10]\n",
    "\n",
    "    # Vector de configuraciones que vamos a utilizar\n",
    "    configuraciones = []\n",
    "\n",
    "    # Guardamos todas las configuraciones en un vector que va a ser el que iteramos con el cluster\n",
    "    for arch in architectures\n",
    "        for opt in optimizers\n",
    "            for batch_size in batchs_size\n",
    "                push!(configuraciones, (arch, opt, batch_size))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Path donde se encuentran los datos de las señales\n",
    "    path_read = \"/home/juan.morales/datos_PCA\"\n",
    "\n",
    "    # Tomamos 1000 puntos de t hasta 0.1 s y 100 puntos de t después de 0.1 s\n",
    "    t_short = collect(range(0, 0.1, length = 1000))\n",
    "    t_long = collect(range(0.1, 1, length = 100))\n",
    "    \n",
    "    # Concatenamos los tiempos para tener un muestreo completo\n",
    "    t = vcat(t_short, t_long)\n",
    "    \n",
    "    # Vamos a tomar un subconjunto de t para hacer el entrenamiento de la NODE para agilizar los tiempos de entrenamiento\n",
    "    # Esto indica cuantos puntos nos salteamos para tomar un subconjunto de t desde el inicio hasta el final de ambos muestreos\n",
    "    muestreo_corto = 50 \n",
    "    muestreo_largo = 4\n",
    "    # Esto da como resultado 20 puntos de t hasta 0.1 s y 25 puntos de t después de 0.1 s\n",
    "    t_short = t_short[1:muestreo_corto:end]\n",
    "    t_long = t_long[1:muestreo_largo:end]\n",
    "    \n",
    "    t = vcat(t_short, t_long)\n",
    "\n",
    "    # Voy a tomar 100 señales elegidas tomando lcm de 5 en 5 hasta 5.45 μm y manteniendo σ = 1.0 con el muestreo de tiempo previamente definido\n",
    "    rango = 100:500:50000\n",
    "    Signals_test, Signals_test_derivadas, column_lcm_test, column_sigmass_test = Get_Signals_Data_Training(path_read, rango, lcms, sigmas, muestreo_corto, muestreo_largo)\n",
    "\n",
    "    # Indice del punto desde el cual vamos a hacer la predicción\n",
    "    idxforecast = 23\n",
    "\n",
    "    # Tomamos los tiempos de entrenamiento y de predicción\n",
    "    tforecast = t[idxforecast:end]\n",
    "    ttrain = t[1:idxforecast-1]\n",
    "\n",
    "    # Tomamos las señales de entrenamiento y de predicción\n",
    "    Signals_test_train = Signals_test[:,1:22]\n",
    "    Signals_test_valid = Signals_test[:,23:end]\n",
    "\n",
    "    # Tomamos las derivadas de las señales de entrenamiento\n",
    "    Signals_test_derivadas_train = Signals_test_derivadas[1:22,:]\n",
    "\n",
    "    # Tomamos un learning rate de 0.005\n",
    "    eta = 5e-3\n",
    "\n",
    "    # Vamos a tomar 1500 épocas para entrenar todas las arquitecturas\n",
    "    epochs = 1500\n",
    "\n",
    "    # Todas las señales tienen la misma condición inicial U0 = 1\n",
    "    U0 = ones32(size(Signals_test)[1])\n",
    "\n",
    "    # Vamos a iterar sobre todas las configuraciones y vamos a entrenar una NODE con mini-batchs para cada una de ellas\n",
    "    architecture, opt, batch_size = configuraciones[parse(Int128,ARGS[1])]\n",
    "\n",
    "    layers = architecture[1]\n",
    "    activation = architecture[2]\n",
    "\n",
    "    if activation == tanh\n",
    "        activation_string = \"tanh\"\n",
    "    elseif activation == relu\n",
    "        activation_string = \"relu\"\n",
    "    elseif activation == swish\n",
    "        activation_string = \"swish\"\n",
    "    end\n",
    "\n",
    "    if opt == AdamW\n",
    "        opt_string = \"AdamW\"\n",
    "    elseif opt == RMSProp\n",
    "        opt_string = \"RMSProp\"\n",
    "    end\n",
    "\n",
    "    # Vamos a crear el dataloader para el entrenamiento de la NODE con mini-batchs\n",
    "    train_loader = Flux.Data.DataLoader((Signals_test_train, ttrain), batchsize = batch_size)\n",
    "\n",
    "    # Vamos a crear el modelo de la red neuronal que va a estar dentro de la ODE\n",
    "    nn = create_model(layers, activation)\n",
    "    \n",
    "    # Entrenamos una NODE con mini-batchs para cada arquitectura, optimizador y tamaño de mini-batch y guardamos el loss y los parametros de la red neuronal\n",
    "    architecture_loss, theta, loss_forecast = Train_Neural_ODE(nn, U0, Signals_test_derivadas_train, epochs, train_loader, opt, eta, Signals_test_train, Signals_test_valid, ttrain, tforecast)\n",
    "\n",
    "    actual_id = parse(Int128,ARGS[1])\n",
    "    actual_layer = string(layers)\n",
    "    actual_activation = activation_string\n",
    "    actual_optimizer = opt_string\n",
    "    actual_loss_final_train = architecture_loss[end]\n",
    "    actual_loss_final_forecast = loss_forecast[end]\n",
    "    actual_batch_size = batch_size\n",
    "\n",
    "    # Guardamos los resultados en un archivo csv\n",
    "    df_results_total = DataFrame(ID = actual_id, Arquitectura = actual_layer, Activación = actual_activation, Optimizador = actual_optimizer, Batch_Size = actual_batch_size, Loss_Final_Entrenamiento = actual_loss_final_train, Loss_Final_Predicción = actual_loss_final_forecast)\n",
    "    CSV.write(\"/home/juan.morales/Paralel_training_NODE_batching/Resultados/$(actual_id)_$(actual_layer)_$(actual_activation)_$(actual_optimizer)_$(actual_batch_size).csv\", df_results_total)\n",
    "    \n",
    "    # Guardamos los loss y los parametros de la red neuronal en archivos csv\n",
    "    Loss_Matrix = zeros((length(architecture_loss), 2))\n",
    "\n",
    "    for i in 1:length(architecture_loss)\n",
    "        Loss_Matrix[i,1] = architecture_loss[i]\n",
    "        Loss_Matrix[i,2] = loss_forecast[i]\n",
    "    end\n",
    "\n",
    "    df_losses = DataFrame(Loss_Matrix, :auto)\n",
    "\n",
    "    rename!(df_losses, Symbol(\"x1\") => Symbol(\"Loss_Entrenamiento\"))\n",
    "    rename!(df_losses, Symbol(\"x2\") => Symbol(\"Loss_Predicción\"))\n",
    "\n",
    "    CSV.write(\"/home/juan.morales/Paralel_training_NODE_batching/Losses/$(actual_id)_losses.csv\", df_losses)\n",
    "\n",
    "    df_theta = DataFrame(reshape(theta, length(theta), 1), :auto)\n",
    "\n",
    "    CSV.write(\"/home/juan.morales/Paralel_training_NODE_batching/Parameters/$(actual_id)_Parameters.csv\", df_theta)\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados obtenidos que se presentaron con la mejor red en este entrenamiento no fueron buenos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 -Exploración para un conjunto de señales mas complejo pero representativo del dataset que tenemos\n",
    "\n",
    "* Creamos nuevas funciones para cargar las señales de Hahn para poder tener los $\\sigma$ que queremos y las $l_{cm}$ que queremos, con esto creamos un conjuno de señales representativo de las señales que tenemos en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Señales tomadas](../Notebooks/PCA_SignalsRepresent_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este caso tenemos 60 señales con $\\sigma = \\{0.01,~0.2,~0.4,~0.6,~0.8,~1\\}$ y $l_{cm} = \\{0.5,~0.75,~1.0,~1.25,~1.5,~1.75,~2.0,~2.25,~2.5,~2.75\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevas funciónes de utilidades para seleccionar un conjunto de señales representativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que toma las señales representativas para un conjunto de sigmas y lcms además las muestrea según el muestreo corto y largo\n",
    "function Get_Rep_Signals(indexes, sampled_sigmas, lcm_range, path_read, muestreo_corto, muestreo_largo)\n",
    "    dataSignals = Float32.(GetSignals(path_read))\n",
    "    Signals_rep = Float32.(Matrix(dataSignals[:,indexes]'))\n",
    "    Signals_rep_short = Signals_rep[:,1:muestreo_corto:1000]\n",
    "    Signals_rep_long = Signals_rep[:,1001:muestreo_largo:end]\n",
    "    Signals_rep = hcat(Signals_rep_short, Signals_rep_long)\n",
    "    return Signals_rep\n",
    "end\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Función que devuelve señales de prueba, sus derivadas y los parámetros con los que se generaron\n",
    "function Get_Signals_Data_Training(path_read, lcms, sigmas, sampled_sigmas, lcm_range, muestreo_corto, muestreo_largo, t)\n",
    "    # Obtenemos primero los parámetros con los que se generaron las señales\n",
    "    column_lcm, column_sigmas = Get_Signals_Test_Parameters(lcms,sigmas)\n",
    "    df_SignalsParams = DataFrame(\n",
    "        sigmas = column_sigmas,\n",
    "\t    lcm = column_lcm,\n",
    "\t)\n",
    "    indexes = []\n",
    "    # A partir de ciertos sigmas dados por sampled_sigmas y un rango de lcm dado por lcm_range, obtenemos las señales representativas\n",
    "    for sigma in sampled_sigmas\n",
    "        find_rows = findall(x -> x == sigma, df_SignalsParams.sigmas)[lcm_range]\n",
    "        push!(indexes, find_rows)\n",
    "    end\n",
    "    # Indices de estas señales\n",
    "    indexes = vcat(indexes...)\n",
    "    # Obtenemos las señales representativas ya muestreadas\n",
    "    Signals_rep = Get_Rep_Signals(indexes, sampled_sigmas, lcm_range, path_read, muestreo_corto, muestreo_largo)\n",
    "    # Derivadas de las señales\n",
    "    Signals_rep_derivadas = zeros(size(Signals_rep))\n",
    "    for i in 1:size(Signals_rep)[1]\n",
    "        Signals_rep_derivadas[i,:] = derivate_signals(t,Signals_rep[i,:])\n",
    "    end\n",
    "    Signals_rep_derivadas = Float32.(Matrix(Signals_rep_derivadas'))\n",
    "    # Normalizamos las derivadas\n",
    "    for i in 1:size(Signals_rep)[1]\n",
    "        Signals_rep_derivadas[:,i] = Signals_rep_derivadas[:,i] ./ maximum(abs.(Signals_rep_derivadas[:,i]))\n",
    "    end\n",
    "    column_lcm_rep = column_lcm[indexes]\n",
    "    column_sigmas_rep = column_sigmas[indexes]\n",
    "    return Signals_rep, Signals_rep_derivadas, column_lcm_rep, column_sigmas_rep\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El programa no difiere mucho del anterior, solo que ahora se seleccionan las señales de Hahn de manera mas representativa y se exploraron únicamente 12 señales\n",
    "\n",
    "* Se añade la implementación de una función costo con penalización pensando en que las funciones deben ser estrictamente decrecientes añadiendo el término:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\n",
    "\\lambda \\sum_{j=1}^{\\mu} \\sum_{i=1}^{N-1} max\\left(S^{(j)}(t_{i+1}) - S^{(j)}(t_i),\\: 0\\right),\n",
    "\n",
    "\\end{equation}$$\n",
    "\n",
    "donde $\\lambda$ es un nuevo hiperparámetro del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de entrenamiento nueva donde se añade el parámetro $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train_Neural_ODE (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Función que entrena la NODE con mini-batchs\n",
    "function Train_Neural_ODE(nn, U0, extra_parameters ,num_epochs, train_loader, opt, eta, Signals, Signals_forecast, t, tforecast, lamb = 1.0)\n",
    "    # Tiempo sobre el cual resolver\n",
    "    tspan = (0f0, 1f0)\n",
    "\n",
    "    # Parametrizar indices para poder acceder a los parametros extras de la red neuronal\n",
    "    f(x,p) = round(Int, x * (length(p) - 1)) + 1\n",
    "    \n",
    "    p, re = Flux.destructure(nn) # Para entrenar la red tenemos que extraer los parametros de la red neuronal en su condicion inicial\n",
    "\n",
    "    # Optimizardor\n",
    "    opt = opt(eta)\n",
    "\n",
    "    # Función que resuelve la ODE con los parametros extra y las condiciones iniciales que instanciemos y nos regresa la solución en un arreglo\n",
    "    function predict_NeuralODE(u0, parametros, time_batch)\n",
    "        # dSdt = NN(S, parametros_extra) \n",
    "        function dSdt(u, p, t; parametros_extra = parametros)\n",
    "            indx = f(t, parametros) \n",
    "            parametros_actuales = parametros[indx] # Selecciona los parametros extra en el tiempo t\n",
    "            entrada_red = vcat(u, parametros_actuales) # Concatena los el valor de S(t) con los parametros extra en el tiempo t\n",
    "            return re(p)(entrada_red) # Regresa la salida de la red neuronal re creada con los parámetros p\n",
    "        end\n",
    "\n",
    "        prob = ODEProblem(dSdt, u0, tspan)\n",
    "\n",
    "        return Array(solve(prob, Tsit5(), dtmin=1e-9 , u0 = u0, p = p, saveat = time_batch, reltol = 1e-7, abstol = 1e-7)) # Regresa la solución de la ODE\n",
    "    end\n",
    "\n",
    "    # Función que predice las señales para un conjunto de condiciones iniciales\n",
    "    function Predict_Singals(U0, parametros_extra, time_batch)\n",
    "        Predicted_Signals = zeros(size(time_batch))\n",
    "        for i in 1:length(U0)\n",
    "            u0 = Float32[U0[i]]\n",
    "            predicted_signal = predict_NeuralODE(u0, parametros_extra[:, i], time_batch)[1, :]\n",
    "            Predicted_Signals = hcat(Predicted_Signals, predicted_signal)\n",
    "        end    \n",
    "        Predicted_Signals[:,2:end]\n",
    "    end\n",
    "\n",
    "    # Función de penalización para tratar de mantener la señal monotonamente decrecente\n",
    "    function penalization_term(time_batch,y)\n",
    "        pen = sum(sum.(max.(y[2:end,:] .- y[1:end-1,:], 0)))\n",
    "        return pen\n",
    "    end\n",
    "\n",
    "    # Función de pérdida que vamos a minimizar, recibe un batch de señales y un batch de tiempos\n",
    "    function loss_node(batch, time_batch)\n",
    "        y = Predict_Singals(U0, extra_parameters, time_batch)\n",
    "        y_forecasted = Predict_Singals(U0, extra_parameters, tforecast)\n",
    "        return Flux.mse(y, batch') + lamb * (penalization_term(time_batch, y) + penalization_term(tforecast, y_forecasted))\n",
    "    end\n",
    "    \n",
    "    # Función de callback para guardar el loss cada epoch\n",
    "    global iter = 0\n",
    "    loss = []\n",
    "    loss_forecast = []\n",
    "    callback = function ()\n",
    "        global iter += 1\n",
    "        if iter % (length(train_loader)) == 0\n",
    "            epoch = Int(iter / length(train_loader))\n",
    "            actual_loss = loss_node(Signals, t)\n",
    "            forecast_loss = loss_node(Signals_forecast, tforecast)\n",
    "            println(\"Epoch = $epoch || Loss: $actual_loss || Loss Forecast: $forecast_loss\")\n",
    "            push!(loss, actual_loss)\n",
    "            push!(loss_forecast, forecast_loss)\n",
    "        end\n",
    "        return false\n",
    "    end\n",
    "\n",
    "    # Entrenamos la red neuronal con mini-batchs\n",
    "    Flux.train!(loss_node, Flux.params(p), ncycle(train_loader, num_epochs), opt, cb = callback)\n",
    "    \n",
    "    # Devolvemos el loss final y los parametros de la red neuronal\n",
    "    return loss, p, loss_forecast\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nueva función de main donde se exploran las nuevas redes entrenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "function main()\n",
    "    # Arquitecturas que vamos a utilizar\n",
    "    architectures = [\n",
    "        [[2, 16, 16, 1], relu], # Dos capas ocultas\n",
    "        [[2, 16, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 16, 16, 1], swish], # Misma con activación swish\n",
    "        \n",
    "        [[2, 32, 64, 16, 1], relu], # Tres capas ocultas\n",
    "        [[2, 32, 64, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 32, 64, 16, 1], swish], # Misma con activación swish\n",
    "\n",
    "        [[2, 128, 64, 16, 1], relu], # Tres capas ocultas con mas neuronas\n",
    "        [[2, 128, 64, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 128, 64, 16, 1], swish], # Misma con activación swish\n",
    "        \n",
    "        [[2, 64, 128, 64, 32, 16, 1], relu], # Cinco capas ocultas\n",
    "        [[2, 64, 128, 64, 32, 16, 1], tanh], # Misma con activación tanh\n",
    "        [[2, 64, 128, 64, 32, 16, 1], swish], # Misma con activación swish\n",
    "        ]\n",
    "\n",
    "    # Optimizadores que vamos a utilizar\n",
    "    optimizers = [opt for opt in [AdamW]]\n",
    "\n",
    "    # Numero de mini-batchs que vamos a utilizar \n",
    "    batchs_size = 60 # En este caso cada bath es una señal completa de entrenamiento\n",
    "\n",
    "    # Vector de configuraciones que vamos a utilizar\n",
    "\n",
    "    configuraciones = []\n",
    "\n",
    "    for arch in architectures\n",
    "        for opt in optimizers\n",
    "            for batch_size in batchs_size\n",
    "                push!(configuraciones, (arch, opt, batch_size))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    path_read = \"/home/juan.morales/datos_PCA\"\n",
    "    # path_read = \"C:/Users/Propietario/Desktop/ib/Tesis_V1/Proyecto_Tesis/1-GeneracionDeDatos/Datos_Final/datos_PCA\"\n",
    "\n",
    "    t_short = collect(range(0, 0.1, length = 1000))\n",
    "    t_long = collect(range(0.1, 1, length = 100))\n",
    "        \n",
    "    # Vamos a tomar un subconjunto de t para hacer el entrenamiento de la NODE para agilizar los tiempos de entrenamiento\n",
    "    muestreo_corto = 20 # Cada cuantos tiempos tomamos un timepo para entrenar la NODE\n",
    "    muestreo_largo = 4\n",
    "\n",
    "    # Esto da 75 tiempos 50 puntos desde 0 a 0.1 y 25 puntos desde 0.1 a 1\n",
    "    t_short = t_short[1:muestreo_corto:end]\n",
    "    t_long = t_long[1:muestreo_largo:end]\n",
    "    \n",
    "    t = vcat(t_short, t_long)\n",
    "\n",
    "    # Tomamos 6 sigmas y 10 tamaños de compartimientos para cada sigma o sea 60 señales\n",
    "    sampled_sigmas = [0.01, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    lcm_range = 1:25:250\n",
    "    \n",
    "    # Obtenemos las señales representativas para un conjunto de sigmas y lcms\n",
    "    Signals_rep, Signals_rep_derivadas, column_lcm_rep, column_sigmas_rep = Get_Signals_Data_Training(path_read, lcms, sigmas, sampled_sigmas, lcm_range, muestreo_corto, muestreo_largo, t)\n",
    "\n",
    "    # A partir de que indice de tiempo vamos a tomar predicciones\n",
    "    # Los anteriroes tiempos se usan para entrenar la NODE\n",
    "    idx_forecast = 61\n",
    "\n",
    "    # Tiempos de entrenamiento y de predicción\n",
    "    tforecast = t[idx_forecast:end]\n",
    "    ttrain = t[1:idx_forecast-1]\n",
    "\n",
    "    # Señaes de entrenamiento y de predicción\n",
    "    Signals_train = Signals_rep[:,1:idx_forecast-1]\n",
    "    Signals_valid = Signals_rep[:,idx_forecast:end]\n",
    "\n",
    "    # Derivadas de las señales de entrenamiento y de predicción\n",
    "    Signals_derivadas_train = Signals_rep_derivadas[1:idx_forecast-1,:]\n",
    "    Signals_derivadas_valid = Signals_rep_derivadas[idx_forecast:end,:]\n",
    "\n",
    "    # Tomamos un learning rate de 0.001\n",
    "    eta = 1e-4\n",
    "\n",
    "    # Vamos a tomar 2000 épocas para entrenar todas las arquitecturas\n",
    "    epochs = 2000\n",
    "\n",
    "    # Todas las señales tienen la misma condición inicial U0 = 1\n",
    "    U0 = ones32(size(Signals_rep)[1])\n",
    "\n",
    "    # Para el entrenamiento en el cluster vamos iterando sobre las configuraciones y guardamos los resultados en archivos csv\n",
    "    # architecture, opt, batch_size = configuraciones[1]\n",
    "    architecture, opt, batch_size = configuraciones[parse(Int128,ARGS[1])]\n",
    "\n",
    "    layers = architecture[1]\n",
    "    activation = architecture[2]\n",
    "\n",
    "    if activation == tanh\n",
    "        activation_string = \"tanh\"\n",
    "    elseif activation == relu\n",
    "        activation_string = \"relu\"\n",
    "    elseif activation == swish\n",
    "        activation_string = \"swish\"\n",
    "    end\n",
    "\n",
    "    if opt == AdamW\n",
    "        opt_string = \"AdamW\"\n",
    "    elseif opt == RMSProp\n",
    "        opt_string = \"RMSProp\"\n",
    "    end\n",
    "\n",
    "    # Vamos a crear el dataloader para el entrenamiento de la NODE con mini-batchs\n",
    "    train_loader = Flux.Data.DataLoader((Signals_train, ttrain), batchsize = batch_size)\n",
    "\n",
    "    # Vamos a crear el modelo de la red neuronal que va a estar dentro de la ODE\n",
    "    nn = create_model(layers, activation)\n",
    "    \n",
    "    # Parámetro de penalización\n",
    "    lambd = 0.1\n",
    "\n",
    "    # Entrenamos una NODE con mini-batchs para cada arquitectura, optimizador y tamaño de mini-batch y guardamos el loss y los parametros de la red neuronal\n",
    "    architecture_loss, theta, loss_forecast = Train_Neural_ODE(nn, U0, Signals_derivadas_train, epochs, train_loader, opt, eta, Signals_train, Signals_valid, ttrain, tforecast, lambd)\n",
    "\n",
    "    # println(\"Arquitectura: $architecture\", \" || Optimizador: $opt\", \" || Tamaño de mini-batch: $batch_size\", \" || Loss: $(architecture_loss[end])\", \" || Loss Forecast: $(loss_forecast[end])\")\n",
    "\n",
    "    # actual_id = 1\n",
    "    actual_id = parse(Int128,ARGS[1])\n",
    "    actual_layer = string(layers)\n",
    "    actual_activation = activation_string\n",
    "    actual_optimizer = opt_string\n",
    "    actual_loss_final_train = architecture_loss[end]\n",
    "    actual_loss_final_forecast = loss_forecast[end]\n",
    "    actual_batch_size = batch_size\n",
    "\n",
    "    # Guardamos los resultados en un archivo csv\n",
    "\n",
    "    df_results_total = DataFrame(ID = actual_id, Arquitectura = actual_layer, Activación = actual_activation, Optimizador = actual_optimizer, Batch_Size = actual_batch_size, Loss_Final_Entrenamiento = actual_loss_final_train, Loss_Final_Predicción = actual_loss_final_forecast)\n",
    "\n",
    "    # CSV.write(\"C:/Users/Propietario/Desktop/ib/Tesis_V1/Proyecto_Tesis/3-GeneracionDeSeñales/Exploracion Paralelizada/RepresentativeTrain_NODE/Resultados/$(actual_id)_$(actual_layer)_$(actual_activation)_$(actual_optimizer)_$(actual_batch_size).csv\", df_results_total)\n",
    "    CSV.write(\"/home/juan.morales/Representative_mini_trainingNODE/Resultados/$(actual_id)_$(actual_layer)_$(actual_activation)_$(actual_optimizer)_$(actual_batch_size).csv\", df_results_total)\n",
    "    \n",
    "\n",
    "    # Guardamos los loss y los parametros de la red neuronal en archivos csv\n",
    "\n",
    "    Loss_Matrix = zeros((length(architecture_loss), 2))\n",
    "\n",
    "    for i in 1:length(architecture_loss)\n",
    "        Loss_Matrix[i,1] = architecture_loss[i]\n",
    "        Loss_Matrix[i,2] = loss_forecast[i]\n",
    "    end\n",
    "\n",
    "    df_losses = DataFrame(Loss_Matrix, :auto)\n",
    "\n",
    "    rename!(df_losses, Symbol(\"x1\") => Symbol(\"Loss_Entrenamiento\"))\n",
    "    rename!(df_losses, Symbol(\"x2\") => Symbol(\"Loss_Predicción\"))\n",
    "\n",
    "    # CSV.write(\"C:/Users/Propietario/Desktop/ib/Tesis_V1/Proyecto_Tesis/3-GeneracionDeSeñales/Exploracion Paralelizada/RepresentativeTrain_NODE/Losses/$(actual_id)_losses.csv\", df_losses)\n",
    "    # CSV.write(\"/home/juan.morales/Representative_mini_trainingNODE/Losses/$(actual_id)_losses.csv\", df_losses)\n",
    "    CSV.write(\"/home/juan.morales/Representative_mini_trainingNODE/Losses/11_losses.csv\", df_losses)\n",
    "\n",
    "    df_theta = DataFrame(reshape(theta, length(theta), 1), :auto)\n",
    "    # CSV.write(\"C:/Users/Propietario/Desktop/ib/Tesis_V1/Proyecto_Tesis/3-GeneracionDeSeñales/Exploracion Paralelizada/RepresentativeTrain_NODE/Parameters/$(actual_id)_Parameters.csv\", df_losses)\n",
    "    CSV.write(\"/home/juan.morales/Representative_mini_trainingNODE/Parameters/$(actual_id)_Parameters.csv\", df_theta)\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se resumen los resultados en la siguiente tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "& \\begin{array}{cccccc}\n",
    "\\hline \\hline\n",
    "\\textbf{ID} & \\textbf{Arquitectura} & \\textbf{Activación} & \\textbf{Optimizador} & \\textbf{Batch Size} & \\textbf{Loss Final Entrenamiento} & \\textbf{Loss Final Predicción} \\\\ \\hline\n",
    "1  & 2, 16, 16, 1            & relu                 & AdamW                 & 50                   & 0.15966              & 0.05333               \\\\ \\hline\n",
    "2  & 2, 16, 16, 1            & tanh                 & AdamW                 & 50                   & 0.19056              & 0.07639               \\\\ \\hline\n",
    "4  & 2, 32, 64, 16, 1        & relu                 & AdamW                 & 50                   & 0.07914              & 0.02104               \\\\ \\hline\n",
    "5  & 2, 32, 64, 16, 1        & tanh                 & AdamW                 & 50                   & 0.13468              & 0.01940               \\\\ \\hline\n",
    "6  & 2, 32, 64, 16, 1        & swish                & AdamW                 & 50                   & 0.07149              & 0.02450               \\\\ \\hline\n",
    "7  & 2, 128, 64, 16, 1       & relu                 & AdamW                 & 50                   & 0.06233              & 0.01870               \\\\ \\hline\n",
    "8  & 2, 128, 64, 16, 1       & tanh                 & AdamW                 & 50                   & 0.12185              & 0.01868               \\\\ \\hline\n",
    "9  & 2, 128, 64, 16, 1       & swish                & AdamW                 & 50                   & 0.06562              & 0.01956               \\\\ \\hline\n",
    "10 & 2, 64, 128, 64, 32, 16, 1 & relu                & AdamW                 & 50                   & 0.04986              & 0.01680               \\\\ \\hline\n",
    "12 & 2, 64, 128, 64, 32, 16, 1 & swish               & AdamW                 & 50                   & 0.04708              & 0.01640               \\\\ \\hline\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
