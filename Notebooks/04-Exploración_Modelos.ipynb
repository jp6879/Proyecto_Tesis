{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Exploración de modelos para la predicción de parámetros a partir de las componentes principales de las señales\n",
    "* Programa para la exploración de hiperparámetros de una red neuronal para la predicción de parámetros a partir de las componentes principales de las señales.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traemos los mismos datos de los parametros utilizados para generar los datos de otro programa\n",
    "include(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\Tesis_V1\\\\Proyecto_Tesis\\\\1-GeneracionDeDatos\\\\Parametros.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribucion de probabilidad log-normal se puede utilizar para añadir a la función de costo final, toma demasiado tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Pln(lcm::Float32, σ::Float32)\n",
    "    \"\"\"Función que calcula la probabilidad de un valor de lc dado un valor de lcm y un valor de σ con una distribución lognormal\n",
    "    Parametros: \n",
    "        lcm: valor de la media de la distribución lognormal\n",
    "        σ: valor de la desviación estandar de la distribución lognormal\n",
    "    Retorna:\n",
    "        Un arreglo con la probabilidad de cada valor de lc dado un valor de lcm y un valor de σ\n",
    "    \"\"\"\n",
    "    return [(exp(-(log(lc) - log(lcm))^2 / (2σ^2))) / (lc * σ * sqrt(2π)) for lc in lcs]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mérica de evaluación de las predicciones de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Mean Absolute Error\n",
    "function RMAE(predicted, real)\n",
    "    \"\"\"Función que calcula el error medio absoluto relativo entre dos arreglos\n",
    "    Parametros:\n",
    "        predicted: arreglo con los valores predichos\n",
    "        real: arreglo con los valores reales\n",
    "    Retorna:\n",
    "        El error medio absoluto relativo entre los dos arreglos\n",
    "    \"\"\"\n",
    "\treturn mean(abs.(predicted .- real)) / mean(real)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regularizaciones L1 y L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizaciones L1 y L2 para la red neuronal\n",
    "pen_l2(x::AbstractArray) = Float32.(sum(abs2, x) / 2)\n",
    "pen_l1(x::AbstractArray) = Float32.(sum(abs, x) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear los modelos de la red neuronal\n",
    "function create_model(layers::Vector{Int}, activation)\n",
    "\tactivations = [activation for i in 1:length(layers) - 2]\n",
    "\treturn Chain([Dense(layers[i], layers[i+1], activations[i]) for i in 1:length(layers) - 2]..., Dense(layers[end-1], layers[end], softplus))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar los datos de entrenamiento, validacion\n",
    "function load_data(x_train, y_train, x_valid, y_valid, batchsize::Int, shuffle::Bool)\n",
    "    \"\"\"Función que carga los datos de entrenamiento y validación en un DataLoader de Flux para ser utilizados en la red neuronal\n",
    "    Parametros:\n",
    "        x_train: arreglo con los datos de entrenamiento\n",
    "        y_train: arreglo con las etiquetas de los datos de entrenamiento\n",
    "        x_valid: arreglo con los datos de validación\n",
    "        y_valid: arreglo con las etiquetas de los datos de validación\n",
    "        batchsize: tamaño del batch\n",
    "        shuffle: booleano que indica si se deben mezclar los datos\n",
    "    Retorna:\n",
    "        Dos DataLoaders, uno con los datos de entrenamiento y otro con los datos de validación\n",
    "    \"\"\"\n",
    "    data = Flux.Data.DataLoader((x_train, y_train), batchsize = batchsize, shuffle = shuffle)\n",
    "    data_valid = Flux.Data.DataLoader((x_valid, y_valid), batchsize = batchsize, shuffle = shuffle)\n",
    "    return data, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que evalua la red neuronal globalmente\n",
    "function eval_model(model, x, y)\n",
    "    y_pred = model(x)\n",
    "    rmae = RMAE(y_pred, y)\n",
    "    return rmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que evalua la red neuronal punto a punto\n",
    "\n",
    "function eval_model_point(model, x, y)\n",
    "    y_pred = model(x)\n",
    "    N = length(y_pred[1,:])\n",
    "\n",
    "    rmae_scores = zeros(N)\n",
    "\n",
    "    for i in 1:N\n",
    "        rmae_scores[i] = RMAE(y_pred[:,i], y[:,i])\n",
    "    end\n",
    "    \n",
    "    return rmae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de guardado de datos\n",
    "\n",
    "function save_loss(loss_vector, filename::String)\n",
    "    CSV.write(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\NN(S)_Params_Exploración\\\\Funciones_loss\\\\\"*filename, DataFrame(loss = loss_vector))\n",
    "end\n",
    "\n",
    "# Función para guardar las predicciones del modelo y el RMAE en cada punto\n",
    "\n",
    "function save_predictions(predictions, rmae_scores, filename::String)\n",
    "    df = DataFrame(x1 = predictions[1,:], x2 = predictions[2,:], rmae_scores = rmae_scores)\n",
    "    CSV.write(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\NN(S)_Params_Exploración\\\\Predicciones\\\\\"*filename, df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenar la red neuronal\n",
    "\n",
    "function train_model(model, id::String, epochs::Int, learning_rate, opt, data, data_valid)\n",
    "    \"\"\"Función que entrena un modelo de red neuronal\n",
    "    Parametros:\n",
    "        model: modelo de red neuronal\n",
    "        id: identificador del modelo\n",
    "        epochs: número de épocas\n",
    "        learning_rate: tasa de aprendizaje\n",
    "        opt: optimizador a utilizar\n",
    "        data: DataLoader con los datos de entrenamiento\n",
    "        data_valid: DataLoader con los datos de validación\n",
    "    Retorna:\n",
    "        RMAE global en los datos de entrenamiento y en los datos de validación\n",
    "        (además guarda los datos de las funciones de costo y las predicciones del modelo en cada punto\n",
    "        en archivos .csv)\n",
    "    \"\"\"\n",
    "    η = learning_rate\n",
    "\n",
    "    if opt == ADAM\n",
    "        opt = ADAM(η)\n",
    "    elseif opt == Descent\n",
    "        opt = Descent(η)\n",
    "    elseif opt == RMSProp\n",
    "        opt = RMSProp(η)\n",
    "    end\n",
    "\n",
    "    # Funciones de costo para utilizar\n",
    "    \n",
    "    function loss_mse(x,y)\n",
    "        y_hat = model(x)\n",
    "        return Flux.mse(y_hat, y)\n",
    "    end\n",
    "\n",
    "    losses = []\n",
    "    losses_valid = []\n",
    "    \n",
    "    # Parámetros de la red neuronal\n",
    "    params = Flux.params(model)\n",
    "    \n",
    "    # Definimos una funcion de callback para ver el progreso del entrenamiento\n",
    "    global iter = 0\n",
    "    cb = function()\n",
    "        global iter += 1\n",
    "        if iter % length(data) == 0\n",
    "            epoch = iter ÷ length(data)\n",
    "            actual_loss = loss_mse(data.data[1], data.data[2])\n",
    "            actual_valid_loss = loss_mse(data_valid.data[1], data_valid.data[2])\n",
    "            if epoch % 1000 == 0\n",
    "                println(\"Epoch $epoch || Loss = $actual_loss || Valid Loss = $actual_valid_loss\")\n",
    "            end\n",
    "            push!(losses, actual_loss)\n",
    "            push!(losses_valid, actual_valid_loss)\n",
    "        end\n",
    "    end;\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        train!(loss_mse, params, data, opt, cb = cb)\n",
    "        if epoch % 500 == 0\n",
    "            η = η * 0.2\n",
    "            if opt == ADAM\n",
    "                opt = ADAM(η)\n",
    "            elseif opt == Descent\n",
    "                opt = Descent(η)\n",
    "            elseif opt == RMSProp\n",
    "                opt = RMSProp(η)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Guardamos la función costo en cada época\n",
    "    save_loss(losses, \"loss_train_\"*id*\".csv\")\n",
    "    save_loss(losses_valid, \"loss_valid_\"*id*\".csv\")\n",
    "\n",
    "    # Evaluamos el modelo en los datos de entrenamiento y validación\n",
    "    rmae_global_train = eval_model(model, data.data[1], data.data[2])\n",
    "    rmae_global_valid = eval_model(model, data_valid.data[1], data_valid.data[2])\n",
    "\n",
    "    rmae_scores_train = eval_model_point(model, data.data[1], data.data[2])\n",
    "    rmae_scores_valid = eval_model_point(model, data_valid.data[1], data_valid.data[2])\n",
    "\n",
    "    # Guardamos las predicciones del modelo y el RMAE en cada punto\n",
    "    save_predictions(model(data.data[1]), rmae_scores_train, \"predictions_train_\"*id*\".csv\")\n",
    "    save_predictions(model(data_valid.data[1]), rmae_scores_valid, \"predictions_valid_\"*id*\".csv\")\n",
    "\n",
    "    # Devolvemos los RMAE globales para globales\n",
    "\n",
    "    return rmae_global_train, rmae_global_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que lee los datos de PCA\n",
    "function read_data_PCA(path_read::String, step_valid::Int64)\n",
    "    \"\"\"Función que lee los datos de PCA generados por el programa de generación de datos\n",
    "    Parametros:\n",
    "        path_read: ruta donde se encuentran los datos\n",
    "        step_valid: paso de los datos de validación\n",
    "    Retorna:\n",
    "        Los datos de señales, los datos de parámetros, los datos de señales de validación y los datos de parámetros de validación\n",
    "    \"\"\"\n",
    "    df_datasignals = CSV.read(path_read * \"\\\\df_PCA_Signals.csv\", DataFrame)\n",
    "    num_datos = size(df_datasignals)[1]\n",
    "    k = 7 # Comienzo de los datos de validación\n",
    "    datasignals_valid = Float32.(Matrix(df_datasignals[k^2:step_valid:num_datos,1:3])')\n",
    "    datasignals = Float32.(Matrix(df_datasignals[setdiff(1:num_datos, k^2:step_valid:num_datos),1:3])')\n",
    "\n",
    "    σ_valid = df_datasignals[k^2:step_valid:num_datos,4]\n",
    "    lcm_valid = df_datasignals[k^2:step_valid:num_datos,5]\n",
    "    \n",
    "    σ_col = df_datasignals[setdiff(1:num_datos, k^2:step_valid:num_datos),4]\n",
    "    lcm_col = df_datasignals[setdiff(1:num_datos, k^2:step_valid:num_datos),5]\n",
    "    \n",
    "    dataparams = hcat(lcm_col, σ_col)'\n",
    "    dataparams_valid = hcat(lcm_valid, σ_valid)'\n",
    "    \n",
    "    return datasignals, dataparams, datasignals_valid, dataparams_valid\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function main()\n",
    "    # Arquitecturas que vamos a utilizar\n",
    "    architectures = [\n",
    "        [[3, 8, 2], relu], # Una capa oculta con pocas neuronas\n",
    "        [[3, 16, 2], relu], # Una capa oculta con más neuronas\n",
    "        [[3, 16, 8, 2], relu], # Dos capas ocultas\n",
    "        [[3, 16, 16, 2], relu], # Dos capas ocultas con aún más neuronas\n",
    "        [[3, 8, 16, 8, 2], relu], # Tres capas ocultas\n",
    "        [[3, 16, 32, 16, 2], relu], # Tres capas ocultas con más neuronas\n",
    "        [[3, 32, 64, 16, 2], relu], # Tres capas ocultas con aun más neuonras\n",
    "        [[3, 16, 32, 16, 8, 2], relu], # Cuatro capas ocultas\n",
    "        [[3, 32, 64, 8, 8, 2], relu], # Cuatro capas ocultas mas neuronas\n",
    "        [[3, 32, 64, 32, 16, 2], relu], # Cuatro capas ocultas con aun mas neuronas\n",
    "        [[3, 30, 25, 20, 15, 10, 2], relu], # Cinco capas ocultas, mayor complejidad\n",
    "        [[3, 16, 8, 2], tanh], # Variando función de activación a tanh\n",
    "        [[3, 16, 32, 16, 2], tanh], # Tres capas ocultas con más neuronas\n",
    "        [[3, 32, 64, 16, 2], tanh], # Tres capas ocultas con aun más neuonras\n",
    "        [[3, 30, 25, 20, 15, 10, 2], σ], # Cinco capas ocultas σ\n",
    "        ]\n",
    "\t\n",
    "    # Optimizadores que vamos a utilizar\n",
    "    optimizers = [opt for opt in [ADAM, RMSProp]]\n",
    "\n",
    "    # Lectura de los datos\n",
    "    path_read = \"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\Datos\\\\Datos_PCA2\"\n",
    "    \n",
    "    # Fraccion de datos que se van a utilizar para validación\n",
    "    percent_valid = 0.1\n",
    "    step_valid = Int(1 / percent_valid)\n",
    "\n",
    "    train_signals, train_params, valid_signals, valid_params = read_data_PCA(path_read, step_valid)\n",
    "\n",
    "    # Cargamos los datos de entrenamiento y validación\n",
    "    batchsize = 64\n",
    "    shuffle = true\n",
    "    data, data_valid = load_data(train_signals, train_params, valid_signals, valid_params, batchsize, shuffle)\n",
    "\n",
    "########### Si se van a hacer mas exploraciones cambiar esto por el id de la ultima arquitectura usada.#################\n",
    "    id = 0\n",
    "\n",
    "    # Función de costo que vamos a utilizar\n",
    "    \n",
    "    id_column = []\n",
    "    layers_column = []\n",
    "    activation_column = []\n",
    "    optimizer_column = []\n",
    "    rmae_global_train_column = []\n",
    "    rmae_global_valid_column = []\n",
    "\n",
    "    for architecture in architectures\n",
    "        for opt in optimizers\n",
    "            id += 1\n",
    "            string_id = string(id)\n",
    "            layers = architecture[1]\n",
    "            activation = architecture[2]\n",
    "            \n",
    "            if activation == σ\n",
    "                activation_string = \"σ\"\n",
    "            elseif activation == tanh\n",
    "                activation_string = \"tanh\"\n",
    "            elseif activation == relu\n",
    "                activation_string = \"relu\"\n",
    "            end\n",
    "\n",
    "            if opt == ADAM\n",
    "                opt_string = \"ADAM\"\n",
    "            elseif opt == Descent\n",
    "                opt_string = \"Descent\"\n",
    "            elseif opt == RMSProp\n",
    "                opt_string = \"RMSProp\"\n",
    "            end\n",
    "\n",
    "            \n",
    "            # Creamos el modelo\n",
    "            model = create_model(layers, activation)\n",
    "\n",
    "            # Definimos el learning rate inicial\n",
    "\n",
    "            learning_rate = 1e-4\n",
    "\n",
    "            # Definimos el número de épocas\n",
    "\n",
    "            epochs = 3000\n",
    "\n",
    "            # Entrenamos el modelo\n",
    "\n",
    "            rmae_global_train, rmae_global_valid = train_model(model, string_id, epochs, learning_rate, opt, data, data_valid)\n",
    "\n",
    "            # Guardamos los datos de la arquitectura\n",
    "\n",
    "            push!(id_column, id)\n",
    "            push!(layers_column, layers)\n",
    "            push!(activation_column, activation_string)\n",
    "            push!(optimizer_column, opt_string)\n",
    "            push!(rmae_global_train_column, rmae_global_train)\n",
    "            push!(rmae_global_valid_column, rmae_global_valid)\n",
    "        end\n",
    "    end\n",
    "    df = DataFrame(id = id_column, layers = layers_column, activation = activation_column, optimizer = optimizer_column, rmae_global_train = rmae_global_train_column, rmae_global_valid = rmae_global_valid_column)\n",
    "    existing_csv_file = \"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\NN(S)_Params_Exploración\\\\Arquitecturas\\\\Registro_arquitecturas.csv\"\n",
    "    if isfile(existing_csv_file)\n",
    "        df_old = CSV.read(existing_csv_file, DataFrame)\n",
    "        df = vcat(df_old, df)\n",
    "    end\n",
    "    CSV.write(\"C:\\\\Users\\\\Propietario\\\\Desktop\\\\ib\\\\5-Maestría\\\\GenData-PCA-UMAP\\\\FNN\\\\NN(S)_Params_Exploración\\\\Arquitecturas\\\\Registro_arquitecturas2.csv\", df)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizó una exploración de 15 arquitecturas distintas, en las que se realizó un aumento secuencial de complejidad variando el número de capas y número de neuronas. Adempás se exploraron algunas de estas redes cambiando la función de activación no lineal en las capas ocultas, dejando softmax en la ultima capa para obtener valores positivios.\n",
    "\n",
    "Todas las arquitecturas fueron entrenadas durante 3000 épocas para mini-batches de 64 elementos, donde se utilizaron los optimizadores ADAM y RNSProp con una tasa de aprendizaje inicial de $1\\times 10^{-4}$ disminuyendola un 80% cada 500 épocas.\n",
    "\n",
    "Al finalizar cada entrenamiento las redes se evaluaron utilizando como métrica global el error relativo medio absoluto (RMAE) definido como\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac{\\frac{1}{N} \\sum_i |σ_{real,i} - σ_{pred,i}| + |{l_{cm}}_{real,i} - {l_{cm}}_{pred,i}|}{\\frac{1}{N} \\sum_i σ_{real,i} + {l_{cm}}_{real,i}},\n",
    "\\end{equation}$\n",
    "\n",
    "donde $N$ es la cantidad de señales de entrenamiento o validación y los sub-indices $real$ y $pred$ inidican los valores rales y predichos por la red neuronal respectivamente.\n",
    "\" \n",
    "md\"Los resultados obtenidos se muestran en la siguiente tabla\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\begin{array}{cccc}\n",
    "\\hline \\hline\n",
    "\\text{layers} & \\text{activation} & \\text{optimizer} & \\text{RMAE Train} & \\text{RMAE Valid} \\\\\n",
    "\\hline\n",
    "3, 8, 2 & relu & ADAM & 0.07033 & 0.06862 \\\\\n",
    "3, 8, 2 & relu & RMSProp & 0.08425 & 0.08284 \\\\\n",
    "3, 16, 2 & relu & ADAM & 0.04570 & 0.04320 \\\\\n",
    "3, 16, 2 & relu & RMSProp & 0.03814 & 0.03624 \\\\\n",
    "3, 16, 8, 2 & relu & ADAM & 0.02236 & 0.02089 \\\\\n",
    "3, 16, 8, 2 & relu & RMSProp & 0.02724 & 0.02613 \\\\\n",
    "3, 16, 16, 2 & relu & ADAM & 0.02243 & 0.02067 \\\\\n",
    "3, 16, 16, 2 & relu & RMSProp & 0.02481 & 0.02295 \\\\\n",
    "3, 8, 16, 8, 2 & relu & ADAM & 0.02432 & 0.02274 \\\\\n",
    "3, 8, 16, 8, 2 & relu & RMSProp & 0.02947 & 0.02815 \\\\\n",
    "3, 16, 32, 16, 2 & relu & ADAM & 0.01414 & 0.01263 \\\\\n",
    "3, 16, 32, 16, 2 & relu & RMSProp & 0.01724 & 0.01582 \\\\\n",
    "3, 32, 64, 16, 2 & relu & ADAM & 0.01365 & 0.01185 \\\\\n",
    "3, 32, 64, 16, 2 & relu & RMSProp & 0.01585 & 0.01436 \\\\\n",
    "3, 16, 32, 16, 8, 2 & relu & ADAM & 0.01355 & 0.01210 \\\\\n",
    "3, 16, 32, 16, 8, 2 & relu & RMSProp & 0.01568 & 0.01368 \\\\\n",
    "3, 32, 64, 8, 8, 2 & relu & ADAM & 0.01221 & 0.0100 \\\\\n",
    "3, 32, 64, 8, 8, 2 & relu & RMSProp & 0.0180 & 0.01622 \\\\\n",
    "3, 32, 64, 32, 16, 2 & relu & ADAM & 0.01326 & 0.01085 \\\\\n",
    "3, 32, 64, 32, 16, 2 & relu & RMSProp & 0.02003 & 0.01761 \\\\\n",
    "3, 30, 25, 20, 15, 10, 2 & relu & ADAM & 0.01316 & 0.01152 \\\\\n",
    "3, 30, 25, 20, 15, 10, 2 & relu & RMSProp & 0.02073 & 0.01859 \\\\\n",
    "3, 16, 8, 2 & tanh & ADAM & 0.02568 & 0.02328 \\\\\n",
    "3, 16, 8, 2 & tanh & RMSProp & 0.02363 & 0.02106 \\\\\n",
    "3, 16, 32, 16, 2 & tanh & ADAM & 0.01575 & 0.01317 \\\\\n",
    "3, 16, 32, 16, 2 & tanh & RMSProp & 0.01707 & 0.01591 \\\\\n",
    "3, 32, 64, 16, 2 & tanh & ADAM & 0.01385 & 0.01244 \\\\\n",
    "3, 32, 64, 16, 2 & tanh & RMSProp & 0.01597 & 0.01286 \\\\\n",
    "3, 30, 25, 20, 15, 10, 2 & \\sigma & ADAM & 0.02373 & 0.02217 \\\\\n",
    "3, 30, 25, 20, 15, 10, 2 & \\sigma & RMSProp & 0.03188 & 0.02899 \\\\\n",
    "3, 32, 64, 8, 8, 2 & swish & ADAM & 0.01682 & 0.01423 \\\\\n",
    "3, 32, 64, 8, 8, 2 & swish & RMSProp & 0.01639 & 0.01359 \\\\\n",
    "3, 32, 64, 32, 16, 2 & swish & ADAM & 0.01267 & 0.01056 \\\\\n",
    "3, 32, 64, 32, 16, 2 & swish & RMSProp & 0.01571 & 0.01250 \\\\\n",
    "3, 30, 25, 20, 15, 10, 2 & swish & ADAM & 0.01406 & 0.01250 \\\\\n",
    "3, 30, 25, 20, 15, 10, 2 & swish & RMSProp & 0.01541 & 0.01273 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$$\n",
    "\n",
    "De este análisis se puede ver que la red que da un mayor rendimiento según la métrica de evaluación es aquella con 4 capas ocultas de 32, 64, 8 y 8 neuronas respectivamente, con la función de activación relu y entrenada con un optimizador ADAM. Otra de las redes con un rendimiento similar es aquella que tiene una mayor cantidad de neuronas en las 4 capas ocultas de 32, 64, 32 y 16 neuronas respectivamente. Estas redes son correspondientes con las filas\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\begin{array}{cccc}\n",
    "\\hline \\hline\n",
    "\\text{layers} & \\text{activation} & \\text{optimizer} & \\text{RMAE Train} & \\text{RMAE Valid} \\\\\n",
    "\\hline\n",
    "3, 32, 64, 8, 8, 2 & \\text{relu} & \\text{ADAM} & 0.01221 & 0.0100 \\\\\n",
    "3, 32, 64, 32, 16, 2 & relu & ADAM & 0.01326 & 0.01085 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Estas puden ser potenciales arquitecuras ópimas para la tarea que buscamos realizar donde se puede hacer una exploración para encontrar los hiperparámetros ópitmos.\n",
    "\n",
    "Para continuar con esta exploración se realizó además una exploración de estas mismas redes usando esta vez los datos agrupados en mini-batchs de 100 elementos obteniendo\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\begin{array}{ccccccc}\n",
    "\\hline \\hline\n",
    "& \\text{Layers} & \\text{Activation} & \\text{Optimizer} & \\text{RMAE Train} & \\text{RMAE Valid} \\\\\n",
    "\\hline\n",
    "& 3, 16, 32, 16, 8, 2 & relu & ADAM & 0.01757 & 0.01521 \\\\\n",
    "& 3, 32, 64, 8, 8, 2 & relu & ADAM & 0.01704 & 0.01593 \\\\\n",
    "& 3, 32, 64, 32, 16, 2 & relu & ADAM & 0.01837 & 0.01591 \\\\\n",
    "& 3, 30, 25, 20, 15, 10, 2 & relu & ADAM & 0.01377 & 0.01222 \\\\\n",
    "& 3, 32, 64, 8, 8, 2 & tanh & ADAM & 0.01494 & 0.01340 \\\\\n",
    "& 3, 32, 64, 32, 16, 2 & tanh & ADAM & 0.01289 & 0.01096 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$$\n",
    "\n",
    "En este caso se obtuvo un rendimiento peor para las redes con las funciones de activación relu pero un rendimiento similar para la última red con función de activación tanh.\n",
    "\n",
    "Por lo observado aumentar el tamaño de los mini-batches no mejora el funcionamiento de las redes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
